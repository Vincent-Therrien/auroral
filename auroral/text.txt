mN_ACTIONS=5classDQN():def__init__(self,network:nn.Module,device:str,frame_size
:int,n_frames:int,n_channels:int,learning_rate:float,batch_size:int,target_upda
te_frequency:int):self.device=deviceself.policy_net=network(frame_size,n_frames
,n_channels).to(self.device)self.target_net=network(frame_size,n_frames,n_chann
els).to(self.device)self.target_net.load_state_dict(self.policy_net.state_dict(
))self.target_net.eval()self.memory=deque(maxlen=2000)self.optimizer=optim.Adam
(self.policy_net.parameters(),lr=learning_rate,weight_decay=1e-4)self.loss_fn=n
n.MSELoss()self.batch_size=batch_sizeself.gamma=0.99self.step_count=0self.targe
t_update_frequency=target_update_frequencyself.current_action=[0for_inrange(N_A
CTIONS)]self.change_action_countdown=0defact(self,state:np.ndarray,epsilon:floa
t):"""Producesanactionbasedonthegameframe.Args:state:Thegamestateasa(256,256,3)
RGBimage.epsilon:Probabilityofchoosingarandomaction.Returns:np.ndarray:Abinaryv
ectorofsize5indicatingthechosenactions."""ifrandom()<epsilon:self.change_action
_countdown-=1ifself.change_action_countdown<0:self.change_action_countdown=3sel
f.current_action=[0for_inrange(N_ACTIONS)]self.current_action[randint(0,len(sel
f.current_action)-1)]=1returnself.current_actionelse:withtorch.no_grad():state=
torch.FloatTensor(state).unsqueeze(0).to(self.device)q_values=self.policy_net(s
tate).tolist()[0]actions=[1ifq==max(q_values)else0forqinq_values]ifactions[-1]:
actions=[1ifq==max(q_values)else0forqinq_values[:-1]]+[1]returnactionsdefq(self
,state:np.ndarray):withtorch.no_grad():state=torch.FloatTensor(state).unsqueeze
(0).to(self.device)q_values=self.policy_net(state).tolist()[0]returnq_valuesdef
save(self,filepath:str)->None:torch.save(self.policy_net.state_dict(),filepath)
defload(self,filepath:str)->None:self.policy_net.load_state_dict(torch.load(fil
epath,weights_only=True))defstep(self,state:np.ndarray,action:list[int],reward:
float,next_state:np.ndarray,done:bool)->None:"""Performsasingletrainingstep.Arg
s:state(np.ndarray):Currentstateooftheenvironment(1frame).action(list):Binaryve
ctoroftheactiontaken.reward(float):Rewardreceivedaftertakingtheaction.next_stat
e(np.ndarray):Nextstate(1frame)done(bool):Whethertheepisodeisfinished."""self.p
olicy_net.train()self.step_count+=1self.memory.append((state,action,reward,next
_state,done))iflen(self.memory)<self.batch_size:returnbatch=sample(self.memory,
self.batch_size)state_batch,action_batch,reward_batch,next_state_batch,done_bat
ch=zip(*batch)state_batch=torch.FloatTensor(np.stack(state_batch,axis=0)).to(se
lf.device)action_batch=torch.LongTensor(np.stack(action_batch,axis=0)).to(self.
device)reward_batch=torch.FloatTensor(np.stack(reward_batch,axis=0)).to(self.de
vice)next_state_batch=torch.FloatTensor(np.stack(next_state_batch,axis=0)).to(s
elf.device)done_batch=torch.FloatTensor(np.stack(done_batch,axis=0)).to(self.de
vice)#ComputeQ-valuesforcurrentstatesq_values=self.policy_net(state_batch)q_val
ues=action_batch*q_valuesq_values=torch.sum(q_values,dim=1)#ComputetargetQ-valu
esusingthetargetnetworkwithtorch.no_grad():max_next_q_values=self.target_net(ne
xt_state_batch).max(dim=1)[0]target_q_values=reward_batch+self.gamma*max_next_q
_values*(1-done_batch)#Computelossloss=self.loss_fn(q_values,target_q_values)#O
ptimizeself.optimizer.zero_grad()loss.backward()torch.nn.utils.clip_grad_norm_(
self.policy_net.parameters(),max_norm=10)self.optimizer.step()#Updatetargetnetw
orkifself.step_count%self.target_update_frequency==0:self.target_net.load_state
_dict(self.policy_net.state_dict())classDQN_1_shallow(nn.Module):"""Single-fram
edeepQnetwork.Expectedshapeoftheinput:[batch_size,n_channels*n_frames,n,n],wher
enisthesizeoftheimage."""def__init__(self,frame_size:int,n_frames:int,n_channel
s:int):"""Args:frame_size:Dimensionoftheinputimage.n_frames:Numberofimagesinthe
input.n_channels:Numberofchannelsintheinput-either1or3."""super(DQN_1_shallow,s
elf).__init__()self.conv=nn.Conv2d(in_channels=n_frames*n_channels,out_channels
=32,#Numberoffiltersinthefirstlayerkernel_size=8,#Sizeoftheconvolutionalkernels
tride=4,#Stridefordownsamplingpadding=0#Nopadding)output_size=int((frame_size-8
)/4+1)self.fc1=nn.Linear(32*output_size*output_size,512)self.fc2=nn.Linear(512,
N_ACTIONS)self.sigmoid=nn.Sigmoid()defforward(self,x):x=self.conv(x)x=F.relu(x)
x=x.view(x.size(0),-1)x=self.fc1(x)x=F.relu(x)x=self.fc2(x)x=self.sigmoid(x)ret
urnxclassDQN_1_mid(nn.Module):"""DeepQnetwork."""def__init__(self,frame_size:in
t,n_frames:int,n_channels:int):"""Args:frame_size:Dimensionoftheinputimage.n_fr
ames:Numberofimagesintheinput.n_channels:Numberofchannelsintheinput-either1or3.
"""super(DQN_1_mid,self).__init__()self.conv1=nn.Conv2d(in_channels=n_frames*n_
channels,out_channels=32,kernel_size=8,stride=4,padding=0)output_size=int((fram
e_size-8)/4+1)self.conv2=nn.Conv2d(in_channels=32,out_channels=48,kernel_size=4
,stride=2,padding=0)output_size=int((output_size-4)/2+1)self.conv3=nn.Conv2d(in
_channels=48,out_channels=48,kernel_size=3,stride=1,padding=0)output_size=int((
output_size-3)/1+1)self.drop1=nn.Dropout(p=0.1)self.fc1=nn.Linear(48*output_siz
e*output_size,512)self.drop2=nn.Dropout(p=0.1)self.fc2=nn.Linear(512,N_ACTIONS)
self.output=nn.Softmax(dim=1)defforward(self,x):x=self.conv1(x)x=F.relu(x)#x=se
lf.pool1(x)x=self.conv2(x)x=F.relu(x)x=self.conv3(x)x=F.relu(x)x=x.view(x.size(
0),-1)x=self.drop1(x)x=self.fc1(x)x=F.relu(x)x=self.drop2(x)x=self.fc2(x)x=self

